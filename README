------------------------------------------------------------------------------
------------------------------------------------------------------------------
Cloud Inference Server
Distributed Machine Learning Computation
------------------------------------------------------------------------------
------------------------------------------------------------------------------

Web server built to serve machine learning inference tasks, allowing 
clients to leverage remote computing resources. Aka allowing my gaming 
computer to support the home infrastructure when I'm not using it.
 
------------------------------------------------------------------------------
Adding Modules:
------------------------------------------------------------------------------

The architecture of this software has been designed from the ground up
to eliminate duplicate code and emphasize simplicity of adding new 
modules. Here's what you need to do. 

1. Import the repo of your module in the parent directory up from this 
   one (src).
2. Make sure you have an up-to-date kotakee_companion repo if you're
   using it's code for. 
3. Define a new endpoint in service_definitions. 
4. Create a new module in one of the module subfolders. 
5. Add an entry in handler_definitions for your hanlder pointing out
   the name of the class of your new module + the function to use. 

And that's it!

------------------------------------------------------------------------------
Usage:
------------------------------------------------------------------------------

To host the cloud inference server, make sure you follow these steps:

1. Install pytorch. I'm assuming you're hosting this on a machine with
   a decent GPU for machine learning inference, so go to pytorch.org
   and get the best command relative to your CUDA version. 

2. Install everything required. Run:

   pip install -r requirements.txt 
